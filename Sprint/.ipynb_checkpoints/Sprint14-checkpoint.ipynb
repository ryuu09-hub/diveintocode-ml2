{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint14\n",
    "## ディープラーニングフレームワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】公式Exampleを分担して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFLowの公式Exampleのresearch（定番のモデルから最新のモデルまで多様なコードが公開されている）ディレクトリから\n",
    "\"adversarial_crypto\"を選択した。下記参考Git,文献である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[adversarial_crypto](https://github.com/tensorflow/models/tree/master/research/adversarial_crypto)\n",
    "\n",
    "[Learning to Protect Communications with Adversarial Neural Cryptography](https://arxiv.org/abs/1610.06918)\n",
    "\n",
    "[論文PDF](https://arxiv.org/pdf/1610.06918.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adversarial_crypto: 通信の自動暗号化\n",
    "\n",
    "通信保護とデータ・セキュリティに応用できるモデル\n",
    "特定の暗号アルゴリズムを規定しておらず、EndToEndの反復トレーニングにより、ニューラルネットワークがどのように暗号化と復号化を行うかを学習を実証\n",
    "\n",
    "ニューラルネットワークが秘密鍵を使用して他のニューラルネットワークから情報を保護することを学習できるかどうかを確認します。 (公開鍵を使って暗号化すると秘密鍵でのみ復号できます)\n",
    "具体的には、マルチエージェントシステムで機密性のプロパティを確保することに焦点を当て、それらのプロパティを敵の観点から指定します。 したがって、システムはアリスとボブという名前のニューラルネットワークで構成され、イブという名前の3番目のニューラルネットワークがアリスとボブ間の通信を盗聴することから学習するものを制限することを目指しています。\n",
    "これらのニューラルネットワークに特定の暗号アルゴリズムを処方することはありません。 \n",
    "代わりに、エンドツーエンドで敵対的にトレーニングします。 \n",
    "ニューラルネットワークは、暗号化と復号化の形式を実行する方法、および機密性の目標を満たすためにこれらの操作を選択的に適用する方法を学習できることを示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ニューラルネットワークと暗号化及び機密性の確保**\n",
    "（2016年10月２４日Google公表）\n",
    "\n",
    "**エンドツーエンドにおける三人のネットワーク構成**\n",
    "- アリス：メッセージ送信者。ボブに暗号化したメッセージを送りたい。\n",
    "- ボブ：メッセージ受信者。アリスから受け取った暗号文を復号したい。\n",
    "- イヴ：受動的な攻撃者（メッセージの改ざんはしない）。アリスからボブへの通信を傍受し、暗号文を復号したい。\n",
    "\n",
    "**通信手順**\n",
    "- アリスは平文Pを暗号文Cに暗号化しボブに送る。\n",
    "- ボブは暗号文Cを正確に複合し、複合文PBobを得たい。\n",
    "- イブも暗号文Cを傍受し複合文PEveを得たい。\n",
    "\n",
    "**登場人物をニューラルネットワークとして考える**\n",
    "ここで、アリス、ボブ、イブの各登場人物をニューラルネットワークAlice,Bob,Eveとする。\n",
    "- Aliceは平文Pとvを入力として受け取り暗号文Cを出力する\n",
    "- Bobは暗号文Cと秘密鍵Kを入力として受け取り複合文PBobを出力する\n",
    "- Eveは暗号文Cを入力として受け取り複合文PEveを出力する\n",
    "\n",
    "**point**\n",
    "暗号化の際に、特定の暗号アルゴリズムを指定して用いている訳ではない点。 \n",
    "既知のアルゴリズムを用いていないため、攻撃者もニューラルネットワークを構築して複合化を試みる。 \n",
    "また、攻撃者のニューラルネットワークEveは第三者的立場としての最善のモデルを使用することを仮定する。\n",
    "AliceとBobの間には、Eveには見えないワンタイムパッドを共有。このワンタイムパッドでは、送信ごとに浮動小数点の暗号キーが生成される。AliceとBob双方には、このワンタイムパッドを使って暗号化するよう指示したが、具体的な方法については指示をしていない。共有のワンタイムパッドを使ったXOR(排他的理論和)暗号化が一般的だが、そういった機能の実装については指示しなかった。\n",
    "\n",
    "\n",
    "**学習**\n",
    "- 全結合層を最初の層にもつ\n",
    "- 全結合層の後に畳み込み層のシーケンスが続き、最終的に平文または暗号文に適した サイズの出力を生成する\n",
    "\n",
    "BobとEveには、この暗号を解読するよう指示、Bobは共有のワンタイムパッド（鍵）にアクセスできるが、Aliceがどういった暗号化手段を使っているのか分からないため、学習していく必要がある。\n",
    "16bitのプレーンテキストを送信する際に、4,096個の例を用いて機械学習させていったところ、Bobは7,000回を超えた辺りからbitの誤りが急激に低下し、15,000回以降、0に近い値で推移したのに対し、Eveは最良時でも約5bit、平均して7～8bitの誤りがあったという。（半角文字１byte=8bit）\n",
    "\n",
    "つまり、Bobは共有のワンタイムパッドから暗号化アルゴリズムを見出し、Aliceから送られた暗号をほぼ解読できたが、ワンタイムパッドが共有されていないEveでは内容を推測することしかできず、解読できなかったわけだ。\n",
    "Googleは、こうした研究結果は、ニューラルネットワークは自身の通信を暗号化するだけでなく、外部攻撃からの保護にも活用できるとしている。\n",
    "\n",
    "現代のRSA暗号やSSL暗号は、コンピュータの速度の制限によって強度を担保している暗号システムであるため、計算速度が猛烈に早い量子コンピュータが本格的に実用化した場合、秒で解読できる。\n",
    "ニューラル暗号は、コンピュータの計算速度に依存せず、復号化キーなければわからない。 　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コードの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 08:00:49.834367 4552873408 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Batch size:  4096\n",
      "#       Iter\t     Bob_Recon_Error\t     Eve_Recon_Error\n",
      "         0\t                8.01\t                7.98\n",
      "       200\t                7.79\t                7.38\n",
      "       400\t                6.07\t                6.22\n",
      "       600\t                4.29\t                5.89\n",
      "       800\t                2.26\t                5.84\n",
      "      1000\t                1.44\t                6.07\n",
      "      1200\t                1.00\t                6.27\n",
      "      1400\t                0.73\t                6.46\n",
      "      1600\t                0.57\t                6.52\n",
      "      1800\t                0.46\t                6.72\n",
      "      2000\t                0.38\t                6.77\n",
      "      2200\t                0.32\t                6.86\n",
      "      2400\t                0.27\t                6.97\n",
      "      2600\t                0.23\t                7.03\n",
      "      2800\t                0.19\t                7.07\n",
      "      3000\t                0.17\t                7.11\n",
      "      3200\t                0.15\t                7.15\n",
      "      3400\t                0.13\t                7.24\n",
      "      3600\t                0.12\t                7.21\n",
      "      3800\t                0.11\t                7.24\n",
      "      4000\t                0.10\t                7.28\n",
      "      4200\t                0.09\t                7.34\n",
      "      4400\t                0.08\t                7.32\n",
      "      4600\t                0.07\t                7.38\n",
      "      4800\t                0.07\t                7.34\n",
      "      5000\t                0.06\t                7.39\n",
      "      5200\t                0.05\t                7.44\n",
      "      5400\t                0.05\t                7.38\n",
      "      5600\t                0.04\t                7.41\n",
      "      5800\t                0.04\t                7.42\n",
      "      6000\t                0.04\t                7.47\n",
      "      6200\t                0.04\t                7.41\n",
      "      6400\t                0.03\t                7.42\n",
      "      6600\t                0.03\t                7.51\n",
      "      6800\t                0.03\t                7.42\n",
      "      7000\t                0.03\t                7.39\n",
      "      7200\t                0.03\t                7.47\n",
      "      7400\t                0.03\t                7.45\n",
      "      7600\t                0.02\t                7.42\n",
      "      7800\t                0.02\t                7.46\n",
      "      8000\t                0.02\t                7.50\n",
      "      8200\t                0.02\t                7.42\n",
      "      8400\t                0.02\t                7.48\n",
      "      8600\t                0.02\t                7.44\n",
      "      8800\t                0.02\t                7.45\n",
      "      9000\t                0.02\t                7.43\n",
      "      9200\t                0.01\t                7.45\n",
      "      9400\t                0.01\t                7.52\n",
      "      9600\t                0.01\t                7.50\n",
      "      9800\t                0.01\t                7.47\n",
      "     10000\t                0.02\t                7.39\n",
      "     10200\t                0.01\t                7.49\n",
      "     10400\t                0.01\t                7.50\n",
      "     10600\t                0.01\t                7.47\n",
      "     10800\t                0.01\t                7.53\n",
      "     11000\t                0.01\t                7.52\n",
      "     11200\t                0.01\t                7.51\n",
      "     11400\t                0.01\t                7.61\n",
      "     11600\t                0.01\t                7.56\n",
      "     11800\t                0.01\t                7.55\n",
      "     12000\t                0.01\t                7.56\n",
      "     12200\t                0.01\t                7.55\n",
      "     12400\t                0.01\t                7.52\n",
      "     12600\t                0.01\t                7.56\n",
      "     12800\t                0.01\t                7.51\n",
      "     13000\t                0.01\t                7.47\n",
      "     13200\t                0.01\t                7.48\n",
      "     13400\t                0.01\t                7.41\n",
      "     13600\t                0.01\t                7.48\n",
      "     13800\t                0.01\t                7.41\n",
      "     14000\t                0.01\t                7.51\n",
      "     14200\t                0.01\t                7.51\n",
      "     14400\t                0.01\t                7.49\n",
      "     14600\t                0.01\t                7.54\n",
      "     14800\t                0.00\t                7.54\n",
      "     15000\t                0.00\t                7.50\n",
      "     15200\t                0.00\t                7.56\n",
      "     15400\t                0.01\t                7.50\n",
      "     15600\t                0.01\t                7.51\n",
      "     15800\t                0.01\t                7.45\n",
      "     16000\t                0.01\t                7.51\n",
      "     16200\t                0.01\t                7.44\n",
      "     16400\t                0.01\t                7.45\n",
      "     16600\t                0.01\t                7.49\n",
      "     16800\t                0.01\t                7.46\n",
      "     17000\t                0.01\t                7.45\n",
      "     17200\t                0.01\t                7.48\n",
      "     17400\t                0.01\t                7.50\n",
      "     17600\t                0.00\t                7.57\n",
      "     17800\t                0.01\t                7.50\n",
      "     18000\t                0.01\t                7.38\n",
      "     18200\t                0.01\t                7.39\n",
      "     18400\t                0.01\t                7.33\n",
      "     18600\t                0.01\t                7.37\n",
      "     18800\t                0.01\t                7.33\n",
      "     19000\t                0.02\t                7.21\n",
      "     19200\t                0.01\t                7.41\n",
      "     19400\t                0.01\t                7.47\n",
      "     19600\t                0.01\t                7.52\n",
      "     19800\t                0.01\t                7.48\n",
      "     20000\t                0.01\t                7.38\n",
      "     20200\t                0.01\t                7.33\n",
      "     20400\t                0.01\t                7.32\n",
      "     20600\t                0.02\t                7.15\n",
      "     20800\t                0.02\t                7.19\n",
      "     21000\t                0.02\t                7.19\n",
      "     21200\t                0.02\t                7.18\n",
      "     21400\t                0.02\t                7.11\n",
      "     21600\t                0.02\t                7.09\n",
      "     21800\t                0.03\t                7.12\n",
      "     22000\t                0.02\t                7.21\n",
      "     22200\t                0.02\t                7.23\n",
      "     22400\t                0.02\t                7.26\n",
      "     22600\t                0.01\t                7.47\n",
      "     22800\t                0.01\t                7.35\n",
      "     23000\t                0.02\t                7.40\n",
      "     23200\t                0.01\t                7.34\n",
      "     23400\t                0.01\t                7.43\n",
      "     23600\t                0.01\t                7.57\n",
      "     23800\t                0.01\t                7.60\n",
      "     24000\t                0.01\t                7.63\n",
      "     24200\t                0.01\t                7.52\n",
      "     24400\t                0.01\t                7.61\n",
      "     24600\t                0.01\t                7.57\n",
      "     24800\t                0.01\t                7.53\n",
      "     25000\t                0.01\t                7.58\n",
      "     25200\t                0.01\t                7.52\n",
      "     25400\t                0.00\t                7.57\n",
      "     25600\t                0.01\t                7.57\n",
      "     25800\t                0.01\t                7.60\n",
      "     26000\t                0.01\t                7.52\n",
      "     26200\t                0.00\t                7.52\n",
      "     26400\t                0.01\t                7.57\n",
      "     26600\t                0.01\t                7.46\n",
      "     26800\t                0.01\t                7.54\n",
      "     27000\t                0.01\t                7.52\n",
      "     27200\t                0.01\t                7.56\n",
      "     27400\t                0.01\t                7.57\n",
      "     27600\t                0.01\t                7.57\n",
      "     27800\t                0.01\t                7.53\n",
      "     28000\t                0.01\t                7.51\n",
      "     28200\t                0.01\t                7.56\n",
      "     28400\t                0.01\t                7.58\n",
      "     28600\t                0.01\t                7.50\n",
      "     28800\t                0.01\t                7.53\n",
      "     29000\t                0.01\t                7.57\n",
      "     29200\t                0.01\t                7.59\n",
      "     29400\t                0.01\t                7.58\n",
      "     29600\t                0.01\t                7.56\n",
      "     29800\t                0.00\t                7.56\n",
      "     30000\t                0.01\t                7.56\n",
      "     30200\t                0.01\t                7.50\n",
      "     30400\t                0.01\t                7.56\n",
      "     30600\t                0.01\t                7.53\n",
      "     30800\t                0.01\t                7.51\n",
      "     31000\t                0.01\t                7.44\n",
      "     31200\t                0.00\t                7.61\n",
      "     31400\t                0.01\t                7.58\n",
      "     31600\t                0.01\t                7.61\n",
      "     31800\t                0.01\t                7.53\n",
      "     32000\t                0.01\t                7.49\n",
      "     32200\t                0.01\t                7.52\n",
      "     32400\t                0.00\t                7.56\n",
      "     32600\t                0.01\t                7.51\n",
      "     32800\t                0.01\t                7.53\n",
      "     33000\t                0.01\t                7.58\n",
      "     33200\t                0.01\t                7.58\n",
      "     33400\t                0.00\t                7.53\n",
      "     33600\t                0.01\t                7.51\n",
      "     33800\t                0.01\t                7.38\n",
      "     34000\t                0.01\t                7.45\n",
      "     34200\t                0.01\t                7.52\n",
      "     34400\t                0.01\t                7.57\n",
      "     34600\t                0.01\t                7.53\n",
      "     34800\t                0.00\t                7.50\n",
      "     35000\t                0.00\t                7.58\n",
      "     35200\t                0.01\t                7.52\n",
      "     35400\t                0.00\t                7.54\n",
      "     35600\t                0.01\t                7.53\n",
      "     35800\t                0.01\t                7.52\n",
      "     36000\t                0.01\t                7.57\n",
      "     36200\t                0.01\t                7.55\n",
      "     36400\t                0.00\t                7.58\n",
      "     36600\t                0.00\t                7.58\n",
      "     36800\t                0.01\t                7.58\n",
      "     37000\t                0.00\t                7.57\n",
      "     37200\t                0.00\t                7.60\n",
      "     37400\t                0.00\t                7.59\n",
      "     37600\t                0.00\t                7.63\n",
      "     37800\t                0.00\t                7.63\n",
      "     38000\t                0.00\t                7.29\n",
      "     38200\t                0.01\t                7.54\n",
      "     38400\t                0.01\t                7.56\n",
      "     38600\t                0.01\t                7.60\n",
      "     38800\t                0.00\t                7.60\n",
      "     39000\t                0.00\t                7.53\n",
      "     39200\t                0.01\t                7.62\n",
      "     39400\t                0.00\t                7.52\n",
      "     39600\t                0.00\t                7.57\n",
      "     39800\t                0.01\t                7.51\n",
      "     40000\t                0.00\t                7.45\n",
      "     40200\t                0.01\t                7.54\n",
      "     40400\t                0.01\t                7.53\n",
      "     40600\t                0.01\t                7.57\n",
      "     40800\t                0.00\t                7.56\n",
      "     41000\t                0.00\t                7.55\n",
      "     41200\t                0.00\t                7.62\n",
      "     41400\t                0.00\t                7.59\n",
      "     41600\t                0.01\t                6.67\n",
      "     41800\t                0.00\t                7.52\n",
      "     42000\t                0.00\t                7.59\n",
      "     42200\t                0.00\t                7.60\n",
      "     42400\t                0.00\t                7.59\n",
      "     42600\t                0.01\t                7.54\n",
      "     42800\t                0.01\t                7.57\n",
      "     43000\t                0.01\t                7.57\n",
      "     43200\t                0.01\t                7.56\n",
      "     43400\t                0.01\t                7.57\n",
      "     43600\t                0.01\t                7.63\n",
      "     43800\t                0.00\t                7.60\n",
      "     44000\t                0.01\t                7.56\n",
      "     44200\t                0.00\t                7.61\n",
      "     44400\t                0.01\t                7.53\n",
      "     44600\t                0.00\t                7.56\n",
      "     44800\t                0.01\t                7.55\n",
      "     45000\t                0.00\t                7.60\n",
      "     45200\t                0.01\t                7.50\n",
      "     45400\t                0.01\t                7.46\n",
      "     45600\t                0.00\t                7.57\n",
      "     45800\t                0.01\t                7.53\n",
      "     46000\t                0.00\t                7.60\n",
      "     46200\t                0.00\t                7.54\n",
      "     46400\t                0.00\t                7.58\n",
      "     46600\t                0.00\t                7.58\n",
      "     46800\t                0.01\t                7.60\n",
      "     47000\t                0.00\t                7.57\n",
      "     47200\t                0.01\t                7.62\n",
      "     47400\t                0.01\t                7.61\n",
      "     47600\t                0.01\t                7.63\n",
      "     47800\t                0.00\t                7.59\n",
      "     48000\t                0.00\t                7.70\n",
      "Target losses achieved.\n",
      "Loss after eve extra training:\n",
      "         0\t                0.00\t                7.53\n",
      "Resetting Eve\n",
      "     10000\t                0.00\t                7.21\n",
      "     20000\t                0.00\t                6.57\n",
      "     30000\t                0.00\t                6.55\n",
      "     40000\t                0.00\t                6.56\n",
      "     50000\t                0.00\t                6.62\n",
      "     60000\t                0.00\t                6.58\n",
      "     70000\t                0.00\t                6.56\n",
      "     80000\t                0.00\t                6.53\n",
      "     90000\t                0.00\t                6.55\n",
      "    100000\t                0.00\t                6.55\n",
      "    110000\t                0.00\t                6.56\n",
      "    120000\t                0.00\t                6.55\n",
      "    130000\t                0.00\t                6.53\n",
      "    140000\t                0.00\t                6.58\n",
      "    150000\t                0.00\t                6.53\n",
      "    160000\t                0.00\t                6.53\n",
      "    170000\t                0.00\t                6.55\n",
      "    180000\t                0.00\t                6.55\n",
      "    190000\t                0.00\t                6.55\n",
      "    200000\t                0.00\t                6.51\n",
      "    210000\t                0.00\t                6.53\n",
      "    220000\t                0.00\t                6.51\n",
      "    230000\t                0.00\t                6.57\n",
      "    240000\t                0.00\t                6.49\n",
      "    250000\t                0.00\t                6.54\n",
      "    250000\t                0.00\t                6.54\n",
      "Resetting Eve\n",
      "     10000\t                0.00\t                7.52\n",
      "     20000\t                0.00\t                6.67\n",
      "     30000\t                0.00\t                6.62\n",
      "     40000\t                0.00\t                6.66\n",
      "     50000\t                0.00\t                6.64\n",
      "     60000\t                0.00\t                6.66\n",
      "     70000\t                0.00\t                6.65\n",
      "     80000\t                0.00\t                6.63\n",
      "     90000\t                0.00\t                6.65\n",
      "    100000\t                0.00\t                6.64\n",
      "    110000\t                0.00\t                6.64\n",
      "    120000\t                0.00\t                6.59\n",
      "    130000\t                0.00\t                6.65\n",
      "    140000\t                0.00\t                6.65\n",
      "    150000\t                0.00\t                6.63\n",
      "    160000\t                0.00\t                6.70\n",
      "    170000\t                0.00\t                6.63\n",
      "    180000\t                0.00\t                6.66\n",
      "    190000\t                0.00\t                6.65\n",
      "    200000\t                0.00\t                6.66\n",
      "    210000\t                0.00\t                6.63\n",
      "    220000\t                0.00\t                6.58\n",
      "    230000\t                0.00\t                6.61\n",
      "    240000\t                0.00\t                6.61\n",
      "    250000\t                0.00\t                6.66\n",
      "    250000\t                0.00\t                6.63\n",
      "Resetting Eve\n",
      "     10000\t                0.00\t                7.42\n",
      "     20000\t                0.00\t                6.62\n",
      "     30000\t                0.00\t                6.59\n",
      "     40000\t                0.00\t                6.56\n",
      "     50000\t                0.00\t                6.58\n",
      "     60000\t                0.00\t                6.58\n",
      "     70000\t                0.00\t                6.59\n",
      "     80000\t                0.00\t                6.59\n",
      "     90000\t                0.00\t                6.61\n",
      "    100000\t                0.00\t                6.64\n",
      "    110000\t                0.00\t                6.55\n",
      "    120000\t                0.00\t                6.63\n",
      "    130000\t                0.00\t                6.57\n",
      "    140000\t                0.00\t                6.57\n",
      "    150000\t                0.00\t                6.54\n",
      "    160000\t                0.00\t                6.60\n",
      "    170000\t                0.00\t                6.58\n",
      "    180000\t                0.00\t                6.58\n",
      "    190000\t                0.00\t                6.63\n",
      "    200000\t                0.00\t                6.59\n",
      "    210000\t                0.00\t                6.57\n",
      "    220000\t                0.00\t                6.61\n",
      "    230000\t                0.00\t                6.55\n",
      "    240000\t                0.00\t                6.55\n",
      "    250000\t                0.00\t                6.57\n",
      "    250000\t                0.00\t                6.57\n",
      "Resetting Eve\n",
      "     10000\t                0.00\t                7.54\n",
      "     20000\t                0.00\t                6.74\n",
      "     30000\t                0.00\t                6.64\n",
      "     40000\t                0.00\t                6.63\n",
      "     50000\t                0.00\t                6.62\n",
      "     60000\t                0.00\t                6.63\n",
      "     70000\t                0.00\t                6.66\n",
      "     80000\t                0.00\t                6.64\n",
      "     90000\t                0.00\t                6.65\n",
      "    100000\t                0.00\t                6.61\n",
      "    110000\t                0.00\t                6.60\n",
      "    120000\t                0.00\t                6.58\n",
      "    130000\t                0.00\t                6.59\n",
      "    140000\t                0.00\t                6.65\n",
      "    150000\t                0.00\t                6.69\n",
      "    160000\t                0.00\t                6.65\n",
      "    170000\t                0.00\t                6.67\n",
      "    180000\t                0.00\t                6.66\n",
      "    190000\t                0.00\t                6.65\n",
      "    200000\t                0.00\t                6.70\n",
      "    210000\t                0.00\t                6.62\n",
      "    220000\t                0.00\t                6.65\n",
      "    230000\t                0.00\t                6.64\n",
      "    240000\t                0.00\t                6.67\n",
      "    250000\t                0.00\t                6.67\n",
      "    250000\t                0.00\t                6.67\n",
      "Resetting Eve\n",
      "     10000\t                0.00\t                7.67\n",
      "     20000\t                0.00\t                6.70\n",
      "     30000\t                0.00\t                6.66\n",
      "     40000\t                0.00\t                6.64\n",
      "     50000\t                0.00\t                6.64\n",
      "     60000\t                0.00\t                6.73\n",
      "     70000\t                0.00\t                6.63\n",
      "     80000\t                0.00\t                6.63\n",
      "     90000\t                0.00\t                6.64\n",
      "    100000\t                0.00\t                6.59\n",
      "    110000\t                0.00\t                6.67\n",
      "    120000\t                0.00\t                6.63\n",
      "    130000\t                0.00\t                6.65\n",
      "    140000\t                0.00\t                6.64\n",
      "    150000\t                0.00\t                6.66\n",
      "    160000\t                0.00\t                6.58\n",
      "    170000\t                0.00\t                6.64\n",
      "    180000\t                0.00\t                6.64\n",
      "    190000\t                0.00\t                6.61\n",
      "    200000\t                0.00\t                6.69\n",
      "    210000\t                0.00\t                6.67\n",
      "    220000\t                0.00\t                6.64\n",
      "    230000\t                0.00\t                6.64\n",
      "    240000\t                0.00\t                6.71\n",
      "    250000\t                0.00\t                6.61\n",
      "    250000\t                0.00\t                6.68\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 The TensorFlow Authors All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Adversarial training to learn trivial encryption functions,\n",
    "from the paper \"Learning to Protect Communications with\n",
    "Adversarial Neural Cryptography\", Abadi & Andersen, 2016.\n",
    "https://arxiv.org/abs/1610.06918\n",
    "This program creates and trains three neural networks,\n",
    "termed Alice, Bob, and Eve.  Alice takes inputs\n",
    "in_m (message), in_k (key) and outputs 'ciphertext'.\n",
    "Bob takes inputs in_k, ciphertext and tries to reconstruct\n",
    "the message.\n",
    "Eve is an adversarial network that takes input ciphertext\n",
    "and also tries to reconstruct the message.\n",
    "The main function attempts to train these networks and then\n",
    "evaluates them, all on random plaintext and key values.\n",
    "\"\"\"\n",
    "\n",
    "# TensorFlow Python 3 compatibility\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import signal\n",
    "import sys\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定義用のflagを作成,tf.app.flags.FLAGSを使うと、TensorFlowのPythonファイルを実行する際にパラメタを付与できる\n",
    "flags = tf.app.flags \n",
    "\n",
    "# floatの定義\n",
    "flags.DEFINE_float('learning_rate', 0.0008, 'Constant learning rate')\n",
    "# intの定義\n",
    "flags.DEFINE_integer('batch_size', 4096, 'Batch size')\n",
    "\n",
    "# 参照用のflagを作成\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Input and output configuration.\n",
    "TEXT_SIZE = 16\n",
    "KEY_SIZE = 16\n",
    "\n",
    "# Training parameters.\n",
    "ITERS_PER_ACTOR = 1\n",
    "EVE_MULTIPLIER = 2  # Train Eve 2x for every step of Alice/Bob\n",
    "\n",
    "# Train until either max loops or Alice/Bob \"good enough\":\n",
    "# Alice/Bobが十分になるまで学習を繰り返す\n",
    "MAX_TRAINING_LOOPS = 850000\n",
    "BOB_LOSS_THRESH = 0.02  # Exit when Bob loss < 0.02 and Eve > 7.7 bits\n",
    "EVE_LOSS_THRESH = 7.7\n",
    "\n",
    "# Logging and evaluation.\n",
    "PRINT_EVERY = 200  # In training, log every 200 steps.　トレーニングでは２００ステップごとに記録\n",
    "EVE_EXTRA_ROUNDS = 2000  # At end, train eve a bit more.　最後に、イブをもう少し訓練\n",
    "RETRAIN_EVE_ITERS = 10000  # Retrain eve up to ITERS*LOOPS times.　ITERS * LOOPS回までイブを再トレーニングし\n",
    "RETRAIN_EVE_LOOPS = 25  # With an evaluation each loop　ループごとに評価\n",
    "NUMBER_OF_EVE_RESETS = 5  # And do this up to 5 times with a fresh eve.　洗練されたEveで５回回す\n",
    "# Use EVAL_BATCHES samples each time we check accuracy.　EVAL_BATCHESを使いaccuracy確認\n",
    "EVAL_BATCHES = 1\n",
    "\n",
    "\n",
    "def batch_of_random_bools(batch_size, n):\n",
    "  \"\"\"Return a batch of random \"boolean\" numbers.\n",
    "  Args:\n",
    "    batch_size:  Batch size dimension of returned tensor.\n",
    "    n:  number of entries per batch.\n",
    "  Returns:\n",
    "    A [batch_size, n] tensor of \"boolean\" numbers, where each number is\n",
    "    preresented as -1 or 1.\n",
    "  \"\"\"\n",
    "\n",
    "  as_int = tf.random.uniform([batch_size, n], minval=0, maxval=2, dtype=tf.int32)\n",
    "    expanded_range = (as_int * 2) - 1\n",
    "    \n",
    "    return tf.cast(expanded_range, tf.float32) #tf.cast()でint型→float型へ変換。\n",
    "\n",
    "\n",
    "class AdversarialCrypto(object):\n",
    "  \"\"\"Primary model implementation class for Adversarial Neural Crypto.\n",
    "  This class contains the code for the model itself,\n",
    "  and when created, plumbs the pathways from Alice to Bob and\n",
    "  Eve, creates the optimizers and loss functions, etc.\n",
    "  Attributes:\n",
    "    eve_loss:  Eve's loss function.\n",
    "    bob_loss:  Bob's loss function.  Different units from eve_loss.\n",
    "    eve_optimizer:  A tf op that runs Eve's optimizer.\n",
    "    bob_optimizer:  A tf op that runs Bob's optimizer.\n",
    "    bob_reconstruction_loss:  Bob's message reconstruction loss,\n",
    "      which is comparable to eve_loss.\n",
    "    reset_eve_vars:  Execute this op to completely reset Eve.\n",
    "    \n",
    "    モデルの本体コードが含まれるクラス、実行されるとアリスからボブとイブへ経路が組込まれ、ロスやオプティマイザーが生成される。\n",
    "  \"\"\"\n",
    "\n",
    "  def get_message_and_key(self):\n",
    "    \"\"\"Generate random pseudo-boolean key and message values.\"\"\"\n",
    "    #キーとメッセージ値生成\n",
    "\n",
    "    batch_size = tf.compat.v1.placeholder_with_default(FLAGS.batch_size, shape=[])\n",
    "\n",
    "    in_m = batch_of_random_bools(batch_size, TEXT_SIZE)\n",
    "    in_k = batch_of_random_bools(batch_size, KEY_SIZE)\n",
    "    return in_m, in_k\n",
    "\n",
    "  def model(self, collection, message, key=None):\n",
    "    \"\"\"The model for Alice, Bob, and Eve.  If key=None, the first fully connected layer\n",
    "    takes only the message as inputs.  Otherwise, it uses both the key\n",
    "    and the message.\n",
    "    Args:\n",
    "      collection:  The graph keys collection to add new vars to.\n",
    "      message:  The input message to process.\n",
    "      key:  The input key (if any) to use.\n",
    "    \"\"\"\n",
    "\n",
    "    if key is not None: #キーがある場合\n",
    "        combined_message = tf.concat(axis=1, values=[message, key])\n",
    "        #メッセージとキーを複合\n",
    "    else:#キーがNoneの場合\n",
    "        combined_message = message\n",
    "        #最初に完全結合レイヤーはメッセージのみを入力として受け取る。 \n",
    "\n",
    "    # Ensure that all variables created are in the specified collection.\n",
    "    #作成した変数が指定されたコレクションにあることを確認。\n",
    "    with tf.contrib.framework.arg_scope(\n",
    "        [tf.contrib.layers.fully_connected, tf.contrib.layers.conv2d],\n",
    "        variables_collections=[collection]):\n",
    "\n",
    "        fc = tf.contrib.layers.fully_connected(\n",
    "          combined_message,\n",
    "          TEXT_SIZE + KEY_SIZE,\n",
    "          biases_initializer=tf.constant_initializer(0.0),\n",
    "          activation_fn=None)\n",
    "\n",
    "        # Perform a sequence of 1D convolutions (by expanding the message out to 2D\n",
    "        # and then squeezing it back down).\n",
    "        fc = tf.expand_dims(fc, 2) # 2D\n",
    "        fc = tf.expand_dims(fc, 3) # 3D -- conv2d needs a depth\n",
    "        # 2,1 -> 1,2\n",
    "        conv = tf.contrib.layers.conv2d(\n",
    "          fc, 2, 2, 2, 'SAME', activation_fn=tf.nn.sigmoid)\n",
    "        # 1,2 -> 1, 2\n",
    "        conv = tf.contrib.layers.conv2d(\n",
    "          conv, 2, 1, 1, 'SAME', activation_fn=tf.nn.sigmoid)\n",
    "        # 1,2 -> 1, 1\n",
    "        conv = tf.contrib.layers.conv2d(\n",
    "          conv, 1, 1, 1, 'SAME', activation_fn=tf.nn.tanh)\n",
    "        conv = tf.squeeze(conv, 3)\n",
    "        conv = tf.squeeze(conv, 2)\n",
    "        return conv\n",
    "\n",
    "    def __init__(self):\n",
    "        in_m, in_k = self.get_message_and_key()\n",
    "        encrypted = self.model('alice', in_m, in_k)\n",
    "        decrypted = self.model('bob', encrypted, in_k)\n",
    "        eve_out = self.model('eve', encrypted, None)\n",
    "\n",
    "        self.reset_eve_vars = tf.group(\n",
    "            *[w.initializer for w in tf.compat.v1.get_collection('eve')])\n",
    "\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        # Eve's goal is to decrypt the entire message:\n",
    "        eve_bits_wrong = tf.reduce_sum(\n",
    "            tf.abs((eve_out + 1.0) / 2.0 - (in_m + 1.0) / 2.0), [1])\n",
    "        self.eve_loss = tf.reduce_sum(eve_bits_wrong)\n",
    "        self.eve_optimizer = optimizer.minimize(\n",
    "            self.eve_loss, var_list=tf.compat.v1.get_collection('eve'))\n",
    "\n",
    "        # Alice and Bob want to be accurate...\n",
    "        self.bob_bits_wrong = tf.reduce_sum(\n",
    "            tf.abs((decrypted + 1.0) / 2.0 - (in_m + 1.0) / 2.0), [1])\n",
    "        # ... and to not let Eve do better than guessing.\n",
    "        self.bob_reconstruction_loss = tf.reduce_sum(self.bob_bits_wrong)\n",
    "        bob_eve_error_deviation = tf.abs(float(TEXT_SIZE) / 2.0 - eve_bits_wrong)\n",
    "        # 7-9 bits wrong is OK too, so we squish the error function a bit.\n",
    "        # Without doing this, we often tend to hang out at 0.25 / 7.5 error,\n",
    "        # and it seems bad to have continued, high communication error.\n",
    "        bob_eve_loss = tf.reduce_sum(\n",
    "            tf.square(bob_eve_error_deviation) / (TEXT_SIZE / 2)**2)\n",
    "\n",
    "        # Rescale the losses to [0, 1] per example and combine.\n",
    "        self.bob_loss = (self.bob_reconstruction_loss / TEXT_SIZE + bob_eve_loss)\n",
    "\n",
    "        self.bob_optimizer = optimizer.minimize(\n",
    "            self.bob_loss,\n",
    "            var_list=(tf.compat.v1.get_collection('alice') + tf.compat.v1.get_collection('bob')))\n",
    "\n",
    "\n",
    "def doeval(s, ac, n, itercount):\n",
    "  \"\"\"Evaluate the current network on n batches of random examples.\n",
    "  Args:\n",
    "    s:  The current TensorFlow session\n",
    "    ac: an instance of the AdversarialCrypto class\n",
    "    n:  The number of iterations to run.\n",
    "    itercount: Iteration count label for logging.\n",
    "  Returns:\n",
    "    Bob and Eve's loss, as a percent of bits incorrect.\n",
    "  \"\"\"\n",
    "\n",
    "  bob_loss_accum = 0\n",
    "  eve_loss_accum = 0\n",
    "  for _ in xrange(n):\n",
    "    bl, el = s.run([ac.bob_reconstruction_loss, ac.eve_loss])\n",
    "    bob_loss_accum += bl\n",
    "    eve_loss_accum += el\n",
    "  bob_loss_percent = bob_loss_accum / (n * FLAGS.batch_size)\n",
    "  eve_loss_percent = eve_loss_accum / (n * FLAGS.batch_size)\n",
    "  print('%10d\\t%20.2f\\t%20.2f'%(itercount, bob_loss_percent, eve_loss_percent))\n",
    "  sys.stdout.flush()\n",
    "  return bob_loss_percent, eve_loss_percent\n",
    "\n",
    "\n",
    "def train_until_thresh(s, ac):\n",
    "    for j in xrange(MAX_TRAINING_LOOPS):\n",
    "        for _ in xrange(ITERS_PER_ACTOR):\n",
    "            s.run(ac.bob_optimizer)\n",
    "        for _ in xrange(ITERS_PER_ACTOR * EVE_MULTIPLIER):\n",
    "            s.run(ac.eve_optimizer)\n",
    "        if j % PRINT_EVERY == 0:\n",
    "            bob_avg_loss, eve_avg_loss = doeval(s, ac, EVAL_BATCHES, j)\n",
    "            if (bob_avg_loss < BOB_LOSS_THRESH and eve_avg_loss > EVE_LOSS_THRESH):\n",
    "                print('Target losses achieved.')\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def train_and_evaluate():\n",
    "  \"\"\"Run the full training and evaluation loop.\"\"\"\n",
    "\n",
    "    ac = AdversarialCrypto()\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    with tf.compat.v1.Session() as s:\n",
    "        s.run(init)\n",
    "        print('# Batch size: ', FLAGS.batch_size)\n",
    "        print('# %10s\\t%20s\\t%20s'%(\"Iter\",\"Bob_Recon_Error\",\"Eve_Recon_Error\"))\n",
    "\n",
    "    if train_until_thresh(s, ac):\n",
    "        for _ in xrange(EVE_EXTRA_ROUNDS):\n",
    "            s.run(ac.eve_optimizer)\n",
    "        print('Loss after eve extra training:')\n",
    "            doeval(s, ac, EVAL_BATCHES * 2, 0)\n",
    "        for _ in xrange(NUMBER_OF_EVE_RESETS):\n",
    "            print('Resetting Eve')\n",
    "            s.run(ac.reset_eve_vars)\n",
    "            eve_counter = 0\n",
    "            for _ in xrange(RETRAIN_EVE_LOOPS):\n",
    "                for _ in xrange(RETRAIN_EVE_ITERS):\n",
    "                    eve_counter += 1\n",
    "                    s.run(ac.eve_optimizer)\n",
    "                doeval(s, ac, EVAL_BATCHES, eve_counter)\n",
    "            doeval(s, ac, EVAL_BATCHES, eve_counter)\n",
    "\n",
    "\n",
    "def main(unused_argv):\n",
    "  # Exit more quietly with Ctrl-C.\n",
    "  signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "    train_and_evaluate()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果から\n",
    "\n",
    "\n",
    "現代のRSA暗号やSSL暗号は、コンピュータの速度の制限によって強度を担保している暗号システムであるため、計算速度が猛烈に早い量子コンピュータが本格的に実用化した場合、一瞬で解読される。\n",
    "ニューラル暗号は、コンピュータの計算速度に依存せず、復号化キーなければわからない。 　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】Iris（2値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
    "\"\"\"\n",
    "\n",
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-versicolor'] = 0\n",
    "y[y=='Iris-virginica'] = 1\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, input_shape=(4,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 5,451\n",
      "Trainable params: 5,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 0s 2ms/sample - loss: 0.9648 - acc: 0.4844\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 0s 295us/sample - loss: 0.7601 - acc: 0.5469\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 0s 320us/sample - loss: 0.6639 - acc: 0.5156\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 0s 245us/sample - loss: 0.6510 - acc: 0.5625\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 0s 207us/sample - loss: 0.5684 - acc: 0.7656\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 0s 327us/sample - loss: 0.5527 - acc: 0.7656\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 0s 261us/sample - loss: 0.4938 - acc: 0.7812\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 0s 248us/sample - loss: 0.4683 - acc: 0.7812\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 0s 285us/sample - loss: 0.4842 - acc: 0.7188\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 0s 278us/sample - loss: 0.4524 - acc: 0.7812\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=10,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_proba >0.5, 1, 0)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】Iris（多値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの読み込み\n",
    "dataset_path =\"Iris.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "# データフレームから条件抽出\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "y = np.array(y)\n",
    "X = np.array(X)\n",
    "# ラベルを数値に変換\n",
    "y[y=='Iris-setosa'] = 0\n",
    "y[y=='Iris-versicolor'] = 1\n",
    "y[y=='Iris-virginica'] = 2\n",
    "y = y.astype(np.int)[:, np.newaxis]\n",
    "\n",
    "# trainとtestに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# さらにtrainとvalに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "#OneHot\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train = enc.fit_transform(y_train)\n",
    "y_test = enc.fit_transform(y_test)\n",
    "y_val = enc.fit_transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 19:31:19.031019 4552873408 deprecation.py:506] From /Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(50, input_shape=(4,), activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 5,653\n",
      "Trainable params: 5,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 0s 1ms/sample - loss: 0.6381 - acc: 0.6875\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 0s 216us/sample - loss: 0.3946 - acc: 0.7917\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 0s 292us/sample - loss: 0.2850 - acc: 0.8438\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 0s 412us/sample - loss: 0.2227 - acc: 0.8958\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 0s 163us/sample - loss: 0.1172 - acc: 0.9583\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 0s 269us/sample - loss: 0.1133 - acc: 0.9479\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 0s 178us/sample - loss: 0.1502 - acc: 0.9375\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 0s 257us/sample - loss: 0.1101 - acc: 0.9479\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 0s 260us/sample - loss: 0.0637 - acc: 0.9792\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 0s 287us/sample - loss: 0.0676 - acc: 0.9792\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=10,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba=model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "print(\"accuracy\",accuracy_score(np.argmax(y_test,axis=1),y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】House PricesをKerasで学習"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df[\"SalePrice\"].values\n",
    "y = np.log(y)\n",
    "X = np.log(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 20,801\n",
      "Trainable params: 20,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(200, input_dim=2, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\",optimizer = tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "             metrics=[\"mse\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=1,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4321411340072334\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "print(np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From /Users/wakaichiaki/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 110s 2ms/step - loss: 0.2675 - accuracy: 0.9180 - val_loss: 0.0569 - val_accuracy: 0.9815\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 122s 2ms/step - loss: 0.0851 - accuracy: 0.9749 - val_loss: 0.0418 - val_accuracy: 0.9851\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.0646 - accuracy: 0.9805 - val_loss: 0.0336 - val_accuracy: 0.9880\n",
      "Epoch 4/12\n",
      "58368/60000 [============================>.] - ETA: 3s - loss: 0.0510 - accuracy: 0.9851"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
