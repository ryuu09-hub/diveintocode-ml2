{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Sprint11\n",
    "## 1次元の畳み込みニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train), (X_test,y_test)=mnist.load_data()\n",
    "X_train,X_test=X_train.reshape(-1,784),X_test.reshape(-1,784)\n",
    "X_train,X_test=X_train.astype(float)/255,X_test.astype(float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "y_train=enc.fit_transform(y_train[:,np.newaxis])\n",
    "y_test=enc.transform(y_test[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xavierの初期値\n",
    "\n",
    "\n",
    "class XavierInitializer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma=1/(math.sqrt(n_nodes1))\n",
    "        W=self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        \n",
    "        B=self.sigma*np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "        #イテレーションごとの勾配の二乗和　初期値は０\n",
    "        \n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "            \n",
    "        H_W=(layer.H_W + layer.dW * layer.dW)\n",
    "        H_B=(layer.H_B + layer.dB * layer.dB)\n",
    "        \n",
    "        #０付近で割ると値が膨れ上がるのを防ぐ\n",
    "        H_W[H_W<1]=1\n",
    "        if H_B<1:\n",
    "            H_B=1\n",
    "        \n",
    "        #print(H_W)\n",
    "        #print(H_B)\n",
    "            \n",
    "        layer.W -= (self.lr /np.sqrt(H_W)) * layer.dW\n",
    "        layer.B -= (self.lr /np.sqrt(H_B)) * layer.dB\n",
    "        layer.H_W=H_W\n",
    "        layer.H_B=H_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    def update(self,layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W-=self.lr*layer.dW\n",
    "        layer.B-=self.lr*layer.dB\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \n",
    "    def __init__(self,input_size, filter_size,optimizer, initializer=XavierInitializer,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        padding なし\n",
    "        stride は１固定\n",
    "        \"\"\"\n",
    "        self.padding=0\n",
    "        self.stride=1\n",
    "        \n",
    "        self.input_size=input_size  #入力の特徴量の数　前層のノード数\n",
    "        self.filter_size=filter_size\n",
    "        self.output_size=self.output_s()\n",
    "        self.initializer=initializer()\n",
    "        \n",
    "        #shape: (filter_size, )\n",
    "        #self.W=self.initializer.W(self.input_size, self.filter_size)\n",
    "        self.W=np.array([3,5,7]).astype(np.float)\n",
    "        #shape: (output_size, )\n",
    "        #self.B=self.initializer.B()\n",
    "        self.B=np.array([1]).astype(np.float)\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        \n",
    "        \n",
    "        #AdaGradで使うやつ\n",
    "        self.H_W=0\n",
    "        self.H_B=0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : 入力  １次元配列\n",
    "        \n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        indexes = np.array([np.arange(i, i+self.filter_size) for i in range(self.output_size)]).astype(np.int)\n",
    "        return ((X[indexes])*(self.W.reshape(1,-1))).sum(axis=1) + self.B\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \n",
    "        indexes = np.array([np.arange(i, i+self.filter_size) for i in range(self.output_size)]).astype(np.int)\n",
    "        self.dW=(self.X[indexes] * (dA.reshape(-1,1))).sum(axis=0)\n",
    "        self.dB=dA.sum()\n",
    "        dX=np.zeros(self.X.shape[0])\n",
    "        for i in range(self.output_size):\n",
    "            dX[i:i+self.filter_size]+=dA[i]*self.W\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def output_s(self):\n",
    "        \n",
    "        return (self.input_size+2*self.padding-self.filter_size)//self.stride + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def N_Out(X,W,padding,stride):\n",
    "        return ((X.shape[0]+2*padding-len(W))/stride)+1\n",
    "    \n",
    "#テスト\n",
    "N_Out(x,w,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv=SimpleConv1d(input_size=4, filter_size=3, optimizer=AdaGrad())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35., 50.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "Conv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30. 110. 170. 140.]\n",
      "[ 50  80 110]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "dA=np.array([10, 20])\n",
    "print(Conv.backward(dA))\n",
    "print(Conv.dW)\n",
    "print(Conv.dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_size=3\n",
    "output_size=2\n",
    "indexes = np.array([np.arange(i, i+filter_size) for i in range(output_size)]).astype(np.int)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x[indexes], w)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 50])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x[indexes]*w).sum(axis=1) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA=np.array([10, 20])\n",
    "indexes = np.array([np.arange(i, i+filter_size) for i in range(output_size)]).astype(np.int)\n",
    "dW=(x[indexes]*(dA.reshape(-1,1))).sum(axis=0)\n",
    "dB=dA.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50  80 110]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(dW)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30 50 70]\n",
      "[ 60 100 140]\n"
     ]
    }
   ],
   "source": [
    "dX=np.zeros(x.shape[0])\n",
    "for i in range(output_size):\n",
    "    print(w*dA[i])\n",
    "    dX[i:i+filter_size]+=w*dA[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30., 110., 170., 140.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#工夫\n",
    "a = np.empty((2, 3))\n",
    "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
    "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
    "\n",
    "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
    "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
    "a = a.sum(axis=1)\n",
    "\n",
    "x = np.array([1, 2, 3, 4])\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
    "x[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xavierの初期値\n",
    "#Conv１D用に変更\n",
    "\n",
    "\n",
    "class XavierInitializer_2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, input_size, input_ch,  filter_size, n_filters):\n",
    "        self.sigma=1/(math.sqrt(input_size))\n",
    "        W=self.sigma*np.random.randn(n_filters, input_ch, filter_size)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_filters):\n",
    "        B=self.sigma*np.random.randn(n_filters)\n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad_2():\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "        #イテレーションごとの勾配の二乗和　初期値は０\n",
    "        \n",
    "        \n",
    "    def update(self, layer):\n",
    "        H_W=(layer.H_W + layer.dW * layer.dW)\n",
    "        H_B=(layer.H_B + layer.dB * layer.dB)\n",
    "        \n",
    "        #０付近で割ると値が膨れ上がるのを防ぐ\n",
    "        H_W[H_W<1]=1\n",
    "        if H_B<1:\n",
    "            H_B=1\n",
    "        \n",
    "        #print(H_W)\n",
    "        #print(H_B)\n",
    "            \n",
    "        layer.W -= (self.lr /np.sqrt(H_W)) * layer.dW\n",
    "        layer.B -= (self.lr /np.sqrt(H_B)) * layer.dB\n",
    "        layer.H_W=H_W\n",
    "        layer.H_B=H_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d_nc():\n",
    "    \n",
    "    def __init__(self,input_s, filter_s,  optimizer, initializer=XavierInitializer_2):\n",
    "        \"\"\"\n",
    "        padding なし\n",
    "        stride は１固定\n",
    "        \"\"\"\n",
    "        self.padding=0\n",
    "        self.stride=1\n",
    "        \n",
    "        self.input_s=input_s#(前層のチャンネル、前層のノード) tuple\n",
    "        self.C=self.input_s[0]\n",
    "        self.IS=self.input_s[1]\n",
    "        \n",
    "        self.filter_s=filter_s #filterの（フィルター数、サイズ） tuple\n",
    "        self.FN=self.filter_s[0]\n",
    "        self.FS=self.filter_s[2]\n",
    "        \n",
    "        self.output_s=self.output_size()\n",
    "        self.initializer=initializer()\n",
    "        \n",
    "        #shape: (n_filters, filter_size, )\n",
    "        #self.W=self.initializer.W(self.input_s[1], self.input_s[0], self.filter_s[1], self.filter_s[0])\n",
    "        self.W=np.ones((3, 2, 3))\n",
    "        #shape: (n_filters )\n",
    "        #self.B=self.initializer.B(self.filter_s[0])\n",
    "        self.B=np.array([1, 2, 3])\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        self.indexes = np.array([np.arange(i, i+self.filter_s[2]) for i in range(self.output_s)]).astype(np.int)\n",
    "        \n",
    "        \n",
    "        #AdaGradで使うやつ\n",
    "        self.H_W=0\n",
    "        self.H_B=0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X=X\n",
    "        \n",
    "        X_col=np.zeros((self.output_s, self.FS*self.C))\n",
    "        for i in range(self.C):\n",
    "            X_col[:self.output_s,i*self.FS:i*self.FS+self.FS]=X[i][self.indexes]\n",
    "        w_1=self.W.reshape(-1, self.C*self.FS)\n",
    "        \n",
    "        a=(np.dot(X_col, w_1.T)+self.B).T\n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.dB=dA.sum(axis=1)\n",
    "        \n",
    "        indexes = np.array([np.arange(i, i+self.filter_s[2]) for i in range(self.output_s)]).astype(np.int)\n",
    "\n",
    "        #dX\n",
    "        dX=np.zeros((2,4))\n",
    "        for i in range(self.C):\n",
    "            w_1=w.transpose(1,0,2)\n",
    "            X_col=np.dot(w_1[i].T, dA )\n",
    "            for j in range(self.output_s):\n",
    "                dX[i, j:j+self.FS]+=X_col.T[j,:] \n",
    "                \n",
    "                \n",
    "        #dW        \n",
    "        X_col=np.zeros((self.C, self.FS*self.output_s))\n",
    "        for i in range(self.C):\n",
    "            \n",
    "            X_col[:self.output_s,i*self.FS:i*self.FS+self.FS]=self.X[i][self.indexes]\n",
    "        \n",
    "\n",
    "        D=np.dot(dA, X_col)\n",
    "        self.dW=D.reshape(self.FN, -1, self.FS)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def output_size(self):\n",
    "        \n",
    "        return (self.input_s[1]+2*self.padding-self.filter_s[2])//self.stride + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv=SimpleConv1d_nc(input_s=(2,4), filter_s=(3,2,3),  optimizer=AdaGrad(), initializer=XavierInitializer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[125. 230. 204. 113.]\n",
      " [102. 206. 195. 102.]]\n",
      "[[[ 31.  51.  71.]\n",
      "  [ 51.  71.  91.]]\n",
      "\n",
      " [[102. 169. 236.]\n",
      "  [169. 236. 303.]]\n",
      "\n",
      " [[164. 272. 380.]\n",
      "  [272. 380. 488.]]]\n"
     ]
    }
   ],
   "source": [
    "input_size=4\n",
    "n_filter=3\n",
    "input_ch=2\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "w=np.array([[[1,1,2],[2,1,1]],[[2,1,1],[1,1,1]],[[1,1,1],[1,1,1]]])\n",
    "\n",
    "dA=np.array([[9,11], [32, 35], [52,56]])\n",
    "print(conv.backward(dA))\n",
    "print(conv.dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_size=3\n",
    "output_size=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=np.ones((3,2,3))\n",
    "n_filter=3\n",
    "input_ch=2\n",
    "a=np.zeros((filter_size, output_size))\n",
    "for i in range(n_filter):\n",
    "    A=np.zeros((input_ch,  output_size))\n",
    "    for j in range(input_ch):\n",
    "        A[j, :]=(x[j, :][indexes]*w[i, j, :]).sum(axis=1)\n",
    "    a[i, :]=A.sum(axis=0) +b[i]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#二次元化の考えを用いたforward　間違ってたかも\n",
    "x_col=np.zeros((input_ch, filter_size*output_size))\n",
    "for i in range(input_ch):\n",
    "    x_col[i, :]=x[i][indexes].flatten()\n",
    "\n",
    "w_1=w.reshape(-1, input_ch*filter_size)\n",
    "w_1.T\n",
    "(np.dot(x_col, w_1.T)+b).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#二次元化の考えを用いたforward 改 多分合ってる\n",
    "x_col=np.zeros((output_size, filter_size*input_ch))\n",
    "for i in range(input_ch):\n",
    "    x_col[:output_size, filter_size*i:filter_size*i+filter_size]=x[i][indexes]\n",
    "\n",
    "w_1=w.reshape(-1, input_ch*filter_size)\n",
    "w_1.T\n",
    "(np.dot(x_col, w_1.T)+b).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 2., 3., 4.],\n",
       "       [2., 3., 4., 3., 4., 5.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  2,  3],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 7,  8,  9],\n",
       "        [10, 11, 12]],\n",
       "\n",
       "       [[13, 14, 15],\n",
       "        [16, 17, 18]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1=np.arange(1,19).reshape(3,2,3)\n",
    "w_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  7, 13],\n",
       "       [ 2,  8, 14],\n",
       "       [ 3,  9, 15],\n",
       "       [ 4, 10, 16],\n",
       "       [ 5, 11, 17],\n",
       "       [ 6, 12, 18]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_1.reshape(-1,6).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  9 18]\n",
      "[11 11 22]\n",
      "[18  9  9]\n",
      "[22 11 11]\n",
      "[64 32 32]\n",
      "[70 35 35]\n",
      "[32 32 32]\n",
      "[35 35 35]\n",
      "[52 52 52]\n",
      "[56 56 56]\n",
      "[52 52 52]\n",
      "[56 56 56]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[125., 230., 204., 113.],\n",
       "       [102., 206., 195., 102.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=4\n",
    "n_filter=3\n",
    "input_ch=2\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
    "w=np.array([[[1,1,2],[2,1,1]],[[2,1,1],[1,1,1]],[[1,1,1],[1,1,1]]])\n",
    "\n",
    "dA=np.array([[9,11], [32, 35], [52,56]])\n",
    "dX=np.zeros((input_ch, input_size))\n",
    "for i in range(n_filter):\n",
    "    for j in range(input_ch):\n",
    "        for k in range(output_size):\n",
    "            dX[j, k:k+filter_size]+=dA[i, k]*w[i, j, :]\n",
    "            print(dA[i, k]*w[i, j, :])\n",
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[125., 230., 204., 113.],\n",
       "       [102., 206., 195., 102.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX=np.zeros((2,4))\n",
    "for i in range(input_ch):\n",
    "    w_1=w.transpose(1,0,2)\n",
    "    X_col=np.dot(w_1[i].T, dA )\n",
    "    for j in range(output_size):\n",
    "        dX[i, j:j+filter_size]+=X_col.T[j,:]\n",
    "dX        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 1, 2],\n",
       "        [2, 1, 1],\n",
       "        [1, 1, 1]],\n",
       "\n",
       "       [[2, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.transpose(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 31.,  51.,  71.,  51.,  71.,  91.],\n",
       "       [102., 169., 236., 169., 236., 303.],\n",
       "       [164., 272., 380., 272., 380., 488.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#backward dW\n",
    "x_col=np.zeros((input_ch, filter_size*output_size))\n",
    "for i in range(input_ch):\n",
    "    x_col[i, :]=x[i][indexes].flatten()\n",
    "\n",
    "D=np.dot(dA, x_col)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 31.,  51.,  71.],\n",
       "        [ 51.,  71.,  91.]],\n",
       "\n",
       "       [[102., 169., 236.],\n",
       "        [169., 236., 303.]],\n",
       "\n",
       "       [[164., 272., 380.],\n",
       "        [272., 380., 488.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW=D.reshape(n_filter, -1, filter_size)\n",
    "dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow.keras as keras\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train=X_train.reshape(-1, 784)\n",
    "X_test=X_test.reshape(-1,784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max()) # 1.0\n",
    "print(X_train.min()) # 0.0\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train_one_hot=enc.fit_transform(y_train[:,np.newaxis])\n",
    "y_test_one_hot=enc.transform(y_test[:,np.newaxis])\n",
    "print(y_train.shape)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_train_one_hot.dtype)\n",
    "y_train=y_train_one_hot\n",
    "y_test=y_test_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC():\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.n_nodes1=n_nodes1\n",
    "        self.n_nodes2=n_nodes2\n",
    "        self.initializer=initializer\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        \n",
    "        self.W=self.initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B=self.initializer.B(n_nodes2)\n",
    "        \n",
    "        #AdaGradで使うH\n",
    "        self.H_W=0\n",
    "        self.H_B=0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        A=np.dot(X, self.W)+self.B\n",
    "        self.input=X\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        \n",
    "        Z: 前層の出力\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        # 更新\n",
    "        dZ=np.dot(dA, self.W.T)\n",
    "        self.dW=np.dot(self.input.T, dA)\n",
    "        self.dB=dA.sum(axis=0)\n",
    "        \n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        y: 前層の出力\n",
    "        \"\"\"\n",
    "        #C=y.max()\n",
    "        #y=y-C\n",
    "        return np.exp(y)/(np.exp(y).sum(axis=1).reshape(-1,1))\n",
    "    \n",
    "    def backward(self, Z, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        交差エントロピー誤差と合わせた勾配\n",
    "        Z: softmax関数の出力\n",
    "        y: 正解ラベル\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        dA=Z- y\n",
    "        \n",
    "        return dA\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.input=X\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \n",
    "        boo= (self.input>0).astype(int)\n",
    "        return dZ*boo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heの初期値\n",
    "\n",
    "\n",
    "class HeInitializer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma=math.sqrt(2/n_nodes1)\n",
    "        W=self.sigma*np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        \n",
    "        B=self.sigma*np.random.randn(n_nodes2)\n",
    "        \n",
    "        return B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "        #イテレーションごとの勾配の二乗和　初期値は０\n",
    "        \n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "            \n",
    "        H_W=layer.H_W + layer.dW * layer.dW\n",
    "        H_B=layer.H_B + layer.dB * layer.dB\n",
    "        \n",
    "        #０付近で割ると値が膨れ上がるのを防ぐ\n",
    "        H_W[H_W<1]=1\n",
    "        H_B[H_B<1]=1\n",
    "        \n",
    "        #print(H_W.max())\n",
    "        #print(H_B.max())\n",
    "            \n",
    "        layer.W -= (self.lr /np.sqrt(H_W)) * layer.dW\n",
    "        layer.B -= (self.lr /np.sqrt(H_B)) * layer.dB\n",
    "        layer.H_W=H_W\n",
    "        layer.H_B=H_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練用データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDNN_n():\n",
    "    \"\"\"\n",
    "    DNN\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, activations,  n_epochs=50, batch_size=600, \n",
    "                 verbose = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        layers 層のリスト\n",
    "        activations 活性化関数のリスト　（最後はソフトマックスで、lossは誤差エントロピー）\n",
    "        \"\"\"\n",
    "        self.layers=layers\n",
    "        self.activations=activations\n",
    "        \n",
    "        \n",
    "        self.n_epochs=n_epochs\n",
    "        self.batch_size=batch_size\n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "            or ONE-HOT 化されたやつ渡すか\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証用データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #SimpleInitializer(self.sigma)\n",
    "        #XavierInitializer(self.n_features)\n",
    "        #XavierInitializer(self.n_nodes1)\n",
    "        #XavierInitializer(self.n_nodes2)\n",
    "        \n",
    "        #HeInitializer(self.n_features)\n",
    "        #HeInitializer(self.n_nodes1)\n",
    "        #HeInitializer(self.n_nodes2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.loss=[]\n",
    "        self.val_loss=[]\n",
    "        #ミニバッチを取得\n",
    "        for i in range(self.n_epochs):\n",
    "            \n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                #forward\n",
    "                \n",
    "                Z=self.all_forward(mini_X)\n",
    "                \n",
    "                \n",
    "                #backward\n",
    "                dA = self.activations[-1].backward(Z, mini_y)\n",
    "                dZ = self.layers[-1].backward(dA)\n",
    "                \n",
    "                for j in range(2, len(self.layers)+1):\n",
    "                    \n",
    "                    dA = self.activations[-j].backward(dZ) \n",
    "                    dZ = self.layers[-j].backward(dA)\n",
    "                \n",
    "                \n",
    "            \n",
    "            y_pred=self.all_forward(X)\n",
    "            loss_=self.loss_function(y, y_pred)\n",
    "            self.loss.append(loss_)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                y_val_pred=self.all_forward(X_val)\n",
    "                val_loss_=self.loss_function(y_val, y_val_pred)\n",
    "                self.val_loss.append(val_loss_)\n",
    "                \n",
    "    \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                if X_val is not None:\n",
    "                    print(\"epoch{}     loss: {}   valloss: {}\".format(i+1, loss_, val_loss_))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"epoch{}     loss: {} \".format(i+1, loss_))\n",
    "                \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    " \n",
    "        \n",
    "        pred=self.all_forward(X).argmax(axis=1)\n",
    "        \n",
    "        \n",
    "        return pred\n",
    "    \n",
    "    def all_forward(self, X):\n",
    "        Z=X\n",
    "        for i in range(len(self.layers)):\n",
    "            A = self.layers[i].forward(Z)\n",
    "            Z = self.activations[i].forward(A)\n",
    "            \n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loss_function(self, y_true, y_pred):\n",
    "        \n",
    "        #真数０を防ぐために微小な値を足す\n",
    "        delta=1e-7\n",
    "        L=-(y_true*np.log(y_pred+delta)).sum()/y_pred.shape[0]\n",
    "        \n",
    "        return L\n",
    "    \n",
    "        \n",
    "    def normalize(self, X):\n",
    "        \n",
    "        return (X-X.min())/(X.max()-X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class equal():\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "    \n",
    "    def __init__(self,input_size, filter_size,optimizer, initializer=XavierInitializer,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        padding なし\n",
    "        stride は１固定\n",
    "        \"\"\"\n",
    "        self.padding=0\n",
    "        self.stride=1\n",
    "        \n",
    "        self.input_size=input_size  #入力の特徴量の数　前層のノード数\n",
    "        self.filter_size=filter_size\n",
    "        self.output_size=self.output_s()\n",
    "        self.initializer=initializer()\n",
    "        \n",
    "        #shape: (filter_size, )\n",
    "        self.W=self.initializer.W(self.input_size, self.filter_size)\n",
    "        #self.W=np.array([3,5,7]).astype(np.float)\n",
    "        #shape: (output_size, )\n",
    "        self.B=self.initializer.B()\n",
    "        #self.B=np.array([1]).astype(np.float)\n",
    "        \n",
    "        self.optimizer=optimizer\n",
    "        \n",
    "        \n",
    "        #AdaGradで使うやつ\n",
    "        self.H_W=0\n",
    "        self.H_B=0\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X : 入力  １次元配列\n",
    "        \n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        indexes = np.array([np.arange(i, i+self.filter_size) for i in range(self.output_size)]).astype(np.int)\n",
    "        \n",
    "        X=X.reshape(-1)\n",
    "        return (((X[indexes])*(self.W.reshape(1,-1))).sum(axis=1) + self.B).reshape(1,-1)\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \n",
    "        indexes = np.array([np.arange(i, i+self.filter_size) for i in range(self.output_size)]).astype(np.int)\n",
    "        self.dW=((self.X.reshape(-1)[indexes] * (dA.reshape(-1,1))).sum(axis=0)).reshape(-1)\n",
    "        self.dB=dA.sum()\n",
    "        dX=np.zeros(self.X.shape[1])\n",
    "        for i in range(self.output_size):\n",
    "            \n",
    "            dX[i:i+self.filter_size]+=(dA[:,i][0])*self.W\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def output_s(self):\n",
    "        \n",
    "        return (self.input_size+2*self.padding-self.filter_size)//self.stride + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=784\n",
    "\n",
    "sigma=0.01 #SimpleInitializerの時のみ使用\n",
    "layers=[]\n",
    "activations=[]\n",
    "optimizer=AdaGrad(lr=0.01)\n",
    "\n",
    "layers.append(FC(n_features, 600,HeInitializer(), optimizer=optimizer))\n",
    "activations.append(Relu())\n",
    "layers.append(FC(600, 400, HeInitializer(), optimizer=optimizer))\n",
    "activations.append(Relu())\n",
    "layers.append(SimpleConv1d(input_size=400, filter_size=3,  optimizer=SGD(), initializer=XavierInitializer))\n",
    "activations.append(equal())\n",
    "layers.append(FC(398, 200, HeInitializer(), optimizer=optimizer))\n",
    "activations.append(Relu())\n",
    "layers.append(FC(200, 10, HeInitializer(), optimizer=optimizer))\n",
    "activations.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch1     loss: 2.306488702427792   valloss: 2.437215338269098\n",
      "epoch2     loss: 2.257439028477215   valloss: 2.4428827131276813\n",
      "epoch3     loss: 2.214687570822516   valloss: 2.447761110416316\n",
      "epoch4     loss: 2.173577217563566   valloss: 2.4506020487475273\n",
      "epoch5     loss: 2.131127810567685   valloss: 2.455084710387273\n",
      "epoch6     loss: 2.082991129036466   valloss: 2.458719324753308\n",
      "epoch7     loss: 2.029131627959349   valloss: 2.470463226654713\n",
      "epoch8     loss: 1.9652753336050246   valloss: 2.487767572670138\n",
      "epoch9     loss: 1.8806699278227774   valloss: 2.51175187720385\n",
      "epoch10     loss: 1.7652456761418645   valloss: 2.550254203733086\n",
      "epoch11     loss: 1.6144874894635108   valloss: 2.609234127616318\n",
      "epoch12     loss: 1.4241685267070485   valloss: 2.6906560095297025\n",
      "epoch13     loss: 1.1963413724462388   valloss: 2.8032424483039105\n",
      "epoch14     loss: 0.9485004606593938   valloss: 2.9694636296405688\n",
      "epoch15     loss: 0.7034001469236362   valloss: 3.205190757622647\n",
      "epoch16     loss: 0.49587117920521384   valloss: 3.4845991322764065\n",
      "epoch17     loss: 0.34300324309984387   valloss: 3.793070179398269\n",
      "epoch18     loss: 0.24391014454475438   valloss: 4.093494688378044\n",
      "epoch19     loss: 0.18078095468731303   valloss: 4.3710110617433795\n",
      "epoch20     loss: 0.13950529028529726   valloss: 4.619005156193417\n",
      "epoch21     loss: 0.11145278983875583   valloss: 4.836917385445265\n",
      "epoch22     loss: 0.09156465349432   valloss: 5.029416749141828\n",
      "epoch23     loss: 0.07702232623702725   valloss: 5.1992540661793045\n",
      "epoch24     loss: 0.06600158651300096   valloss: 5.3526083275359575\n",
      "epoch25     loss: 0.057450633244343456   valloss: 5.49073163098301\n",
      "epoch26     loss: 0.05064758446808385   valloss: 5.616024893795902\n",
      "epoch27     loss: 0.04513613062835463   valloss: 5.731564279632747\n",
      "epoch28     loss: 0.04060499858909914   valloss: 5.837534465394646\n",
      "epoch29     loss: 0.03681318339142502   valloss: 5.935597580308156\n",
      "epoch30     loss: 0.03360772910733255   valloss: 6.0274559870205815\n",
      "epoch31     loss: 0.03087038309729076   valloss: 6.1135556502327555\n",
      "epoch32     loss: 0.02849984014506643   valloss: 6.193726489678279\n",
      "epoch33     loss: 0.02643727942536132   valloss: 6.270519674992318\n",
      "epoch34     loss: 0.024629462006455695   valloss: 6.341552649062146\n",
      "epoch35     loss: 0.023027624099042805   valloss: 6.409343356974166\n",
      "epoch36     loss: 0.021604558654617467   valloss: 6.473894401732606\n",
      "epoch37     loss: 0.02033460131595292   valloss: 6.53494924310475\n",
      "epoch38     loss: 0.019189422286880073   valloss: 6.593392847563068\n",
      "epoch39     loss: 0.018155198935082818   valloss: 6.64943572628236\n",
      "epoch40     loss: 0.01721940926124647   valloss: 6.703106902860782\n",
      "epoch41     loss: 0.01636576063778557   valloss: 6.754111794789296\n",
      "epoch42     loss: 0.015585126546799463   valloss: 6.80411652126368\n",
      "epoch43     loss: 0.014871263554404875   valloss: 6.851657155468896\n",
      "epoch44     loss: 0.014213273626084898   valloss: 6.897224677628846\n",
      "epoch45     loss: 0.013605561228081509   valloss: 6.941616466711437\n",
      "epoch46     loss: 0.013043882583386299   valloss: 6.9843341745207725\n",
      "epoch47     loss: 0.012523459833327545   valloss: 7.025325099353192\n",
      "epoch48     loss: 0.01203969980277106   valloss: 7.065012903974734\n",
      "epoch49     loss: 0.01158937975381287   valloss: 7.103911527212364\n",
      "epoch50     loss: 0.011169184975331954   valloss: 7.141007107385909\n"
     ]
    }
   ],
   "source": [
    "dnn=SDNN_n(layers=layers, activations=activations, batch_size=1, n_epochs=50)\n",
    "dnn.fit(X_train[0].reshape(1,-1), y_train[0].reshape(1,-1), X_test[0].reshape(1,-1), y_test[0].reshape(1,-1))\n",
    "y_pred=dnn.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
