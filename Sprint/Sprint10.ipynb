{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint10\n",
    "## ディープニューラルネットワークスクラッチ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ取得と前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(-1,784)\n",
    "X_test=X_test.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.astype(np.float)/255\n",
    "X_test=X_test.astype(np.float)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "y_train_one_hot=enc.fit_transform(y_train[:,np.newaxis])\n",
    "y_test_one_hot=enc.transform(y_test[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチクラス導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練用データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.s_init = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = self.s_init.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = self.s_init.B(self.n_nodes2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"       \n",
    "        self.z=X\n",
    "        A=np.dot(X,self.W)+self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ=np.dot(dA,self.W.T)\n",
    "        \n",
    "        self.dW=np.dot(self.z.T,dA)\n",
    "        self.dB=dA.sum(axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=0.01):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W=self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B=self.sigma*np.random.randint(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W-=self.lr*layer.dW\n",
    "        layer.B-=self.lr*layer.dB\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】活性化関数のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    \"\"\"\n",
    "      \n",
    "    def forward(self,A):\n",
    "        self.A=A\n",
    "        Z=self._Tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self,Z):\n",
    "        dA=Z*(1-self._Tanh(self.A)**2)\n",
    "        return dA\n",
    "    \n",
    "    def _Tanh(self,X):\n",
    "        Z=(np.exp(X)-np.exp(-X))/(np.exp(X)+np.exp(-X))\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,A):\n",
    "        Z=self._softmax(A)\n",
    "        \n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backward(self,Z,y):\n",
    "        dA=Z-y\n",
    "        return dA\n",
    "    \n",
    "    def _softmax(self,A):\n",
    "        Z=np.zeros((A.shape[0],A.shape[1]))\n",
    "        C=np.max(A)\n",
    "        exp=np.exp(A/C)\n",
    "        exp_sum=np.sum(exp,axis=1)\n",
    "        \n",
    "        for i in range(A.shape[0]):\n",
    "            Z[i]=exp[i]/exp_sum[i]\n",
    "        return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】ReLUクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数(ReLU)クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self,A):\n",
    "        self.A=A\n",
    "        return np.maximum(0,A)\n",
    "        \n",
    "    def backward(self,Z):\n",
    "        return Z*np.maximum(np.sin(self.A),0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】重みの初期値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    XavierInitializerクラス\n",
    "    ※シグモイド関数やハイパボリックタンジェント関数のとき\n",
    "    \"\"\"\n",
    "    def __init__(self,n_nodes1):\n",
    "        self.sigma=1/np.sqrt(n_nodes1)\n",
    "       \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W=self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B=self.sigma*np.random.randint(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    HeInitializerクラス\n",
    "    ※ReLUのとき\n",
    "    \"\"\"\n",
    "    def __init__(self,n_nodes1):\n",
    "        self.sigma=np.sqrt(2/n_nodes1)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W=self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B=self.sigma*np.random.randint(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題7】最適化手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    最適化手法（AdaGrad）\n",
    "    ある層の重みやバイアスの更新\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : 更新前の層のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self,lr=0.01):\n",
    "        self.lr=lr\n",
    "        self.hW=1\n",
    "        self.hB=1\n",
    "        \n",
    "    def update(self, layer):\n",
    "        self.hW+=layer.dW\n",
    "        self.hB+=layer.dB\n",
    "        layer.W-=self.lr*(1/np.sqrt(self.hW))*layer.dW\n",
    "        layer.B-=self.lr*(1/np.sqrt(self.hB))*layer.dB\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題8】クラスの完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_nodes1=400,n_nodes2=200,optimizer=SGD,initializer=SimpleInitializer,\n",
    "                 lr=0.01,sigma=0.01,epochs=50,batch_size=20,verbose=True,seed=9):\n",
    "        self.lr=lr\n",
    "        self.sigma=sigma\n",
    "        self.epochs=epochs\n",
    "        self.verbose = verbose\n",
    "        self.n_nodes1 = n_nodes1 # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2 # 2層目のノード数\n",
    "        self.n_layr=3\n",
    "        self.optimizer = optimizer(self.lr)\n",
    "        self.initializer=initializer\n",
    "        self.batch_size=batch_size\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"\n",
    "        学習\n",
    "        parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練用データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        self.n_features=X.shape[1]\n",
    "        self.n_output = y.shape[1] # 出力のクラス数（3層目のノード数）\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), self.optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), self.optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), self.optimizer)\n",
    "        self.activation3 = Softmax()\n",
    "        self.get_mini_batch=GetMiniBatch(X,y,self.batch_size)\n",
    "        self.loss=[]\n",
    "        \n",
    "        #エポック回数分学習\n",
    "        for i in range(self.epochs):\n",
    "            for mini_X_train,mini_y_train in self.get_mini_batch:    \n",
    "                #forward\n",
    "                Z=self._forward(mini_X_train)\n",
    "                #backward\n",
    "                self._backward(Z)\n",
    "                 \n",
    "            #lossの記録\n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                self.loss.append(self._cross(X,y))\n",
    "                print(\"{}epoch:{}\".format(i+1,self.loss[-1]))\n",
    "                \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        y_pred=np.ardmax(self._forward(X),axis=1)\n",
    "        \n",
    "        return y_pred \n",
    "        \n",
    "    def _forward(self,X):\n",
    "        self.A1 = self.FC1.forward(X)\n",
    "        self.Z1 = self.activation1.forward(self.A1)\n",
    "        self.A2 = self.FC2.forward(self.Z1)\n",
    "        self.Z2 = self.activation2.forward(self.A2)\n",
    "        self.A3 = self.FC3.forward(self.Z2)\n",
    "        self.Z3 = self.activation3.forward(self.A3)\n",
    "        \n",
    "        return self.Z3\n",
    "    \n",
    "    def _backward(self,y):\n",
    "        dA3 = self.activation3.backward(self.Z3, y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "        \n",
    "    def _cross(self,X,y):\n",
    "        l=-(1/X.shape[0])*np.sum(y*np.log(self._forward(X)))\n",
    "        \n",
    "        return l \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題9】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DN=ScratchDeepNeuralNetrowkClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch:2.3557647936197363\n",
      "2epoch:2.3557647936197363\n",
      "3epoch:2.3557647936197363\n",
      "4epoch:2.3557647936197363\n",
      "5epoch:2.3557647936197363\n",
      "6epoch:2.3557647936197363\n",
      "7epoch:2.3557647936197363\n",
      "8epoch:2.3557647936197363\n",
      "9epoch:2.3557647936197363\n",
      "10epoch:2.3557647936197363\n",
      "11epoch:2.3557647936197363\n",
      "12epoch:2.3557647936197363\n",
      "13epoch:2.3557647936197363\n",
      "14epoch:2.3557647936197363\n",
      "15epoch:2.3557647936197363\n",
      "16epoch:2.3557647936197363\n",
      "17epoch:2.3557647936197363\n",
      "18epoch:2.3557647936197363\n",
      "19epoch:2.3557647936197363\n",
      "20epoch:2.3557647936197363\n",
      "21epoch:2.3557647936197363\n",
      "22epoch:2.3557647936197363\n",
      "23epoch:2.3557647936197363\n",
      "24epoch:2.3557647936197363\n",
      "25epoch:2.3557647936197363\n",
      "26epoch:2.3557647936197363\n",
      "27epoch:2.3557647936197363\n",
      "28epoch:2.3557647936197363\n",
      "29epoch:2.3557647936197363\n",
      "30epoch:2.3557647936197363\n",
      "31epoch:2.3557647936197363\n",
      "32epoch:2.3557647936197363\n",
      "33epoch:2.3557647936197363\n",
      "34epoch:2.3557647936197363\n",
      "35epoch:2.3557647936197363\n",
      "36epoch:2.3557647936197363\n",
      "37epoch:2.3557647936197363\n",
      "38epoch:2.3557647936197363\n",
      "39epoch:2.3557647936197363\n",
      "40epoch:2.3557647936197363\n",
      "41epoch:2.3557647936197363\n",
      "42epoch:2.3557647936197363\n",
      "43epoch:2.3557647936197363\n",
      "44epoch:2.3557647936197363\n",
      "45epoch:2.3557647936197363\n",
      "46epoch:2.3557647936197363\n",
      "47epoch:2.3557647936197363\n",
      "48epoch:2.3557647936197363\n",
      "49epoch:2.3557647936197363\n",
      "50epoch:2.3557647936197363\n"
     ]
    }
   ],
   "source": [
    "DN.fit(X_train,y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
